{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZionOchayon/Machine-Learning-Deep-Learning-Image-Processing/blob/main/AutoMLbasedNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Live Demo : https://drive.google.com/file/d/15IPqHNnnBZEifhl-_vGqwERu4ULieELc/view?usp=drive_link\n"
      ],
      "metadata": {
        "id": "b0qAgdU2-EDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsEuv4oHISCj",
        "outputId": "34498f46-b69b-4bd2-aac2-05365ce1dab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.31.1-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata<8,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (7.0.1)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.2)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.5.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.0)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.9.0)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.2)\n",
            "Collecting validators<1,>=0.2 (from streamlit)\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.2)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<8,>=1.4->streamlit) (3.17.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Installing collected packages: watchdog, validators, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.42 pydeck-0.8.1b0 smmap-5.0.1 streamlit-1.31.1 validators-0.22.0 watchdog-4.0.0\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.1.3-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgHT1DswIGi_",
        "outputId": "f41fcacf-0ff9-43a9-8b83-18fc15cdbf10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "\n",
        "from openai import OpenAI\n",
        "API_key =\n",
        "client = OpenAI(api_key=API_key)\n",
        "\n",
        "model_id = 'gpt-4-1106-preview'\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import base64\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_diabetes\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras import layers\n",
        "import keras\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "from keras.callbacks import TensorBoard\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import openai\n",
        "from google.colab import files\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from yellowbrick.classifier import ClassificationReport, ConfusionMatrix, ROCAUC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from yellowbrick.classifier import ClassificationReport, ConfusionMatrix, ROCAUC\n",
        "from yellowbrick.regressor import PredictionError, ResidualsPlot, CooksDistance\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn import metrics\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import io\n",
        "from PIL import Image\n",
        "import base64\n",
        "from io import StringIO\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import re\n",
        "from sklearn import metrics\n",
        "from yellowbrick.classifier import ClassificationReport, ConfusionMatrix, ROCAUC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "import time\n",
        "from sklearn import datasets, linear_model, metrics\n",
        "import re\n",
        "from io import BytesIO, StringIO\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn import linear_model\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import ast\n",
        "from sklearn.inspection import permutation_importance\n",
        "import math\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from itertools import cycle\n",
        "#---------------------------------#\n",
        "# Page layout\n",
        "\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    <style>\n",
        "    .stApp {\n",
        "      background-color: #d9edf3;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True\n",
        ")\n",
        "\n",
        "\n",
        "st.markdown(\n",
        "    f\"\"\"\n",
        "    <style>\n",
        "    [data-testid=\"stSidebar\"] {{\n",
        "        background-color: #8ec9dd;\n",
        "    }}\n",
        "    </style>\n",
        "      \"\"\",\n",
        "    unsafe_allow_html=True,\n",
        ")\n",
        "\n",
        "title = 'Auto Machine Learning App'\n",
        "st.title(f':rainbow[{title}]')\n",
        "\n",
        "#---------------------------------#\n",
        "# Sidebar - Collects user input features into dataframe\n",
        "st.sidebar.header('Upload your CSV data')\n",
        "if 'uploaded_file' not in st.session_state:\n",
        "    st.session_state['uploaded_file'] = None\n",
        "if 'df' not in st.session_state:\n",
        "    st.session_state['df'] = None\n",
        "\n",
        "st.session_state['uploaded_file'] = st.sidebar.file_uploader(\"Upload your input CSV file\", type=[\"csv\"])\n",
        "\n",
        "if 'file_id' not in st.session_state:\n",
        "    st.session_state['file_id'] = None\n",
        "if st.session_state['uploaded_file'] is not None:\n",
        "    st.session_state['df'] = pd.read_csv(st.session_state['uploaded_file'])\n",
        "    st.write(st.session_state['df'])\n",
        "    file_id = client.files.create(file=(st.session_state['df'].to_json(), \"rb\"), purpose=\"assistants\")\n",
        "    st.session_state['file_id'] = file_id.id\n",
        "\n",
        "if 'learning_question' not in st.session_state:\n",
        "    st.session_state['learning_question'] = None\n",
        "st.session_state['learning_question'] = st.text_input(r\"$\\textsf{\\Large Learning Question}$\")\n",
        "if 'question_content' not in st.session_state:\n",
        "    st.session_state['question_content'] = ['Model Performance and Evaluation', 'Feature Importance and Impact', 'Model Interpretability', 'Model Generalization and Validation', 'Bias and Fairness', 'Error Analysis', 'Model Updates and Maintenance', 'other']\n",
        "\n",
        "def chatgpt_conversation(conversation_log):\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "      model=model_id,\n",
        "      messages=conversation_log\n",
        "  )\n",
        "  return response\n",
        "\n",
        "if 'conversations' not in st.session_state:\n",
        "    st.session_state['conversations'] = []\n",
        "st.session_state['conversations'].append({\"role\": \"system\", \"content\": \"You are a intelligent assistant.\"})\n",
        "def call_gpt(question):\n",
        "  st.session_state['conversations'].append({'role': 'user', 'content': question})\n",
        "  res = chatgpt_conversation(st.session_state['conversations'])\n",
        "  ans = res.choices[0].message.content\n",
        "  st.session_state['conversations'].append({'role': 'assistant', 'content': ans})\n",
        "  return ans\n",
        "\n",
        "def classification_or_regression(col_name):\n",
        "  if st.session_state['df'][col_name].dtype in ['int64', 'float64']:\n",
        "          # If the number of unique values is low, it could be categorical\n",
        "          if st.session_state['df'][col_name].nunique() < 10:\n",
        "              model_type = 'classification'\n",
        "              return model_type\n",
        "          else:\n",
        "              model_type = 'regression'\n",
        "              return model_type\n",
        "  else:\n",
        "          model_type = 'classification'\n",
        "          return model_type\n",
        "\n",
        "\n",
        "def grid_search_parameters(model_type, models_dic, params_dic, eval_method):\n",
        "  if model_type == 'regression':\n",
        "    models_dic = {\n",
        "      'Random Forest Regressor': RandomForestRegressor(),\n",
        "      'Gradient Boosting Regressor': GradientBoostingRegressor(),\n",
        "      'Support Vector Regressor': SVR(),\n",
        "      'Linear Regression': LinearRegression(),\n",
        "      'Ridge Regression': Ridge(),\n",
        "      'Lasso Regression': linear_model.Lasso(),\n",
        "      'ElasticNet Regression': ElasticNet(),\n",
        "      'Decision tree Regressor': DecisionTreeRegressor()\n",
        "      #,'K-Nearest Neighbors':KNeighborsRegressor()\n",
        "    }\n",
        "\n",
        "    params_dic = {\n",
        "      'Random Forest Regressor':{\n",
        "          'model__n_estimators': [10, 50, 100, 200],\n",
        "          'model__max_depth': [1, 10, 20, 30],\n",
        "          'model__min_samples_split': [2, 5, 10]\n",
        "      },\n",
        "      'Gradient Boosting Regressor':{\n",
        "          'model__learning_rate': [0.001, 0.01, 0.1, 1],\n",
        "          'model__min_samples_split': [2, 5, 10]\n",
        "      },\n",
        "      'Support Vector Regressor':{\n",
        "          'model__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
        "          'model__gamma': [0.001, 0.01, 0.1, 1],\n",
        "          'model__kernel': ['linear', 'rbf']\n",
        "      },\n",
        "      'Linear Regression':{\n",
        "\n",
        "      },\n",
        "      'Ridge Regression':{\n",
        "          'model__alpha': [0.001, 0.01, 0.1, 1],\n",
        "          'model__max_iter': [1, 10, 20, 30]\n",
        "      },\n",
        "      'Lasso Regression':{\n",
        "          'model__alpha': [0.001, 0.01, 0.1, 1],\n",
        "          'model__max_iter': [1, 10, 20, 30]\n",
        "      },\n",
        "      'ElasticNet Regression':{\n",
        "          'model__alpha': [0.001, 0.01, 0.1, 1],\n",
        "          'model__l1_ratio': [0.01, 0.6, 0.9]\n",
        "      },\n",
        "      'Decision tree Regressor':{\n",
        "          'model__max_depth': [2, 5, 15, 20]\n",
        "      },\n",
        "      'K-Nearest Neighbors':{\n",
        "          'model__n_neighbors': [1, 7, 12, 20],\n",
        "          'model__min_samples_split': [2, 5, 7, 10]\n",
        "      }\n",
        "    }\n",
        "\n",
        "    eval_method = 'r2' #or recommended 'neg_mean_squared_error', 'neg_mean_absolute_error'\n",
        "\n",
        "  elif model_type == 'classification':\n",
        "      models_dic = {\n",
        "        'Random Forest Classifier': RandomForestClassifier(),\n",
        "        'Support Vector Classifier': SVC(),\n",
        "        'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "        'Logistic Regression': LogisticRegression(),\n",
        "        'Gradient Boosting Classifier': GradientBoostingClassifier(),\n",
        "        'XGBoost Classifier': xgb.XGBClassifier(),\n",
        "        'Decision Tree Classifier': DecisionTreeClassifier(),\n",
        "        'Naive Bayes': GaussianNB()\n",
        "      }\n",
        "      params_dic = {\n",
        "          'Logistic Regression':{\n",
        "              'model__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
        "              'model__penalty': ['l2']\n",
        "          },\n",
        "          'Support Vector Classifier':{\n",
        "              'model__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
        "              'model__kernel': ['linear', 'rbf'],\n",
        "              'model__probability': [True]\n",
        "\n",
        "          },\n",
        "          'Random Forest Classifier':{\n",
        "              'model__n_estimators': list((10, 50, 100, 200)),\n",
        "              'model__max_depth': list((None, 10, 20, 30)),\n",
        "              'model__min_samples_split': list((2, 5, 10))\n",
        "          },\n",
        "          'K-Nearest Neighbors':{\n",
        "              'model__n_neighbors': [2, 5, 7, 9]\n",
        "          },\n",
        "          'Gradient Boosting Classifier':{\n",
        "              'model__learning_rate': [0.001, 0.1, 1],\n",
        "              'model__max_depth': [1, 50, 100]\n",
        "          },\n",
        "          'XGBoost Classifier':{\n",
        "              'model__n_estimator': [100, 500, 1000],\n",
        "              'model__max_depth': [3, 7, 10],\n",
        "              'model__min_child_weight': [1, 3, 6]\n",
        "          },\n",
        "          'Decision Tree Classifier':{\n",
        "              'model__max_depth': list((1, 10, 20, 30)),\n",
        "              'model__min_samples_split': list((2, 5, 10)),\n",
        "              'model__min_samples_leaf': list((1, 2, 5, 10))\n",
        "          },\n",
        "          'Naive Bayes':{\n",
        "\n",
        "          }\n",
        "      }\n",
        "\n",
        "      class_eval_method = 'f1' #or 'accuracy', 'f1_micro', 'f1_macro', 'f1_weighted', 'roc_auc', 'average_precision', 'neg_log_loss'\n",
        "  return models_dic, params_dic, eval_method\n",
        "\n",
        "\n",
        "def encode_labels(y_train, y_test):\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "    y_test_encoded = label_encoder.transform(y_test)\n",
        "    return y_train_encoded, y_test_encoded, label_encoder\n",
        "\n",
        "def create_label_dict(label_encoder):\n",
        "    \"\"\"\n",
        "    Creates a dictionary mapping encoded labels to original string labels.\n",
        "\n",
        "    :param label_encoder: The LabelEncoder used for encoding the labels.\n",
        "    :return: A dictionary mapping numerical values to string labels.\n",
        "    \"\"\"\n",
        "    label_dict = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
        "    return label_dict\n",
        "\n",
        "def identify_columns_for_onehot(df):\n",
        "    one_hot_encode = []\n",
        "    leave_as_is = []\n",
        "    columns = df.columns.drop(st.session_state['prediction_value'])\n",
        "    for col in columns:\n",
        "        #classification_or_regression\n",
        "        if df[col].dtype == 'object' or classification_or_regression(col) == \"classification\":\n",
        "            unique_values = df[col].unique()\n",
        "\n",
        "            # Check if only two unique values\n",
        "            if len(unique_values) == 2:\n",
        "                # Check if values are 0 & 1 or 'positive' & 'negative'\n",
        "                if set(unique_values) == {0, 1} or set(map(str.lower, unique_values)) == {'positive', 'negative'} or set(map(str.lower, unique_values)) == {'true', 'false'}:\n",
        "                    leave_as_is.append(col)\n",
        "                else:\n",
        "                    one_hot_encode.append(col)\n",
        "            elif len(unique_values) > 2:\n",
        "                one_hot_encode.append(col)\n",
        "            else:\n",
        "                leave_as_is.append(col)\n",
        "\n",
        "    return one_hot_encode, leave_as_is\n",
        "\n",
        "if 'encode_target' not in st.session_state:\n",
        "    st.session_state['encode_target'] = False\n",
        "if 'pred_val_dic' not in st.session_state:\n",
        "    st.session_state['pred_val_dic'] = None\n",
        "if 'y_train_encoded' not in st.session_state:\n",
        "    st.session_state['y_train_encoded'] = None\n",
        "if 'y_test_encoded' not in st.session_state:\n",
        "    st.session_state['y_test_encoded'] = None\n",
        "if 'categorical_features' not in st.session_state:\n",
        "    st.session_state['categorical_features'] = None\n",
        "if 'numeric_features' not in st.session_state:\n",
        "    st.session_state['numeric_features'] = None\n",
        "if 'has_categorical' not in st.session_state:\n",
        "    st.session_state['has_categorical'] = None\n",
        "if 'feature_names' not in st.session_state:\n",
        "    st.session_state['feature_names'] = None\n",
        "\n",
        "def preprocessing(model_type, preprocessor, prediction_value):\n",
        "    columns = st.session_state['df'].columns.drop(prediction_value)\n",
        "    st.session_state['numeric_features'] = [col for col in columns if classification_or_regression(col) == 'regression']\n",
        "    st.session_state['model_type'] = classification_or_regression(prediction_value)\n",
        "\n",
        "    if st.session_state['model_type'] == 'regression':\n",
        "        numeric_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='mean')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ])\n",
        "    elif st.session_state['model_type'] == 'classification':\n",
        "        if classification_or_regression(prediction_value) == 'classification':\n",
        "            # Handle categorical prediction value\n",
        "            numeric_transformer = Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "                ('scaler', StandardScaler())\n",
        "            ])\n",
        "        else:\n",
        "            numeric_transformer = Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "                ('scaler', StandardScaler())\n",
        "            ])\n",
        "    st.session_state['has_categorical'] = any(classification_or_regression(col) == 'classification' for col in columns)\n",
        "    print(\"check has_categorical = \", st.session_state['has_categorical'])\n",
        "\n",
        "    if st.session_state['has_categorical']:\n",
        "\n",
        "      # Assuming df is your DataFrame\n",
        "      columns_to_encode, columns_to_leave = identify_columns_for_onehot(st.session_state['df'])\n",
        "\n",
        "      categorical_transformer_onehot = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "      ])\n",
        "      categorical_transformer_no_onehot = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent'))\n",
        "      ])\n",
        "\n",
        "      preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "         ('num', numeric_transformer, st.session_state['numeric_features']),\n",
        "         ('cat_onehot', categorical_transformer_onehot, columns_to_encode),\n",
        "         ('cat_no_onehot', categorical_transformer_no_onehot, columns_to_leave)\n",
        "       ],\n",
        "        remainder='passthrough'\n",
        "      )\n",
        "\n",
        "    else:\n",
        "      preprocessor = numeric_transformer\n",
        "\n",
        "    if st.session_state['model_type'] == 'classification':\n",
        "        unique_values = st.session_state['df'][prediction_value].unique()\n",
        "\n",
        "        if all(isinstance(val, str) for val in unique_values):\n",
        "          if not all(val.isdigit() for val in unique_values):\n",
        "              st.session_state['encode_target'] = True\n",
        "              st.session_state['y_train'], st.session_state['y_test'], st.session_state['label_encoder'] = encode_labels(st.session_state['y_train'], st.session_state['y_test'])\n",
        "              st.session_state['pred_val_dic'] = create_label_dict(st.session_state['label_encoder'])\n",
        "          else:\n",
        "              st.session_state['y_train'] = st.session_state['y_train'].astype(int)\n",
        "              st.session_state['y_test'] = st.session_state['y_test'].astype(int)\n",
        "\n",
        "    cat_onehot_index = 1  # Assuming 'cat_onehot' is the second transformer\n",
        "    # Fit the transformer\n",
        "    if len(columns_to_encode) > 0:\n",
        "      cat_onehot_transformer = preprocessor.transformers[cat_onehot_index][1]  # Access the Pipeline object\n",
        "      X_train_array = st.session_state['X_train'][columns_to_encode].values\n",
        "      cat_onehot_transformer.fit(X_train_array)\n",
        "    # Fit the preprocessor on the training data only\n",
        "    preprocessor.fit(st.session_state['X_train'])\n",
        "\n",
        "    # Transform training and test data\n",
        "    st.session_state['X_train_processed'] = preprocessor.transform(st.session_state['X_train'])\n",
        "    st.session_state['X_test_processed'] = preprocessor.transform(st.session_state['X_test'])\n",
        "\n",
        "    if st.session_state['has_categorical']:\n",
        "        # Access OneHotEncoder correctly and handle empty columns_to_encode\n",
        "        try:\n",
        "            onehot_encoder = preprocessor.named_transformers_['cat_onehot']['onehot']\n",
        "        except KeyError:  # Handle scikit-learn version compatibility\n",
        "            onehot_encoder = preprocessor.transformers[1][1]  # Assuming 'cat_onehot' is second transformer\n",
        "\n",
        "        if len(columns_to_encode) > 0:\n",
        "            feature_names = onehot_encoder.get_feature_names_out(columns_to_encode)\n",
        "        else:\n",
        "            feature_names = []  # No categorical features, so no new feature names\n",
        "\n",
        "        feature_names = np.concatenate([columns_to_leave, feature_names])\n",
        "        feature_names = np.concatenate([st.session_state['numeric_features'], feature_names])\n",
        "    else:\n",
        "        feature_names = columns  # Use original column names\n",
        "\n",
        "    st.session_state['feature_names'] = feature_names\n",
        "\n",
        "    #Convert transformed data into DataFrame\n",
        "    st.session_state['X_train_processed'] = pd.DataFrame(st.session_state['X_train_processed'], columns=st.session_state['feature_names'])\n",
        "    st.session_state['X_test_processed'] = pd.DataFrame(st.session_state['X_test_processed'], columns=st.session_state['feature_names'])\n",
        "\n",
        "    return preprocessor\n",
        "\n",
        "if 'eval_dic' not in st.session_state:\n",
        "    st.session_state['eval_dic'] = None\n",
        "if 'model_type' not in st.session_state:\n",
        "    st.session_state['model_type'] = None\n",
        "if 'best_model' not in st.session_state:\n",
        "    st.session_state['best_model'] = None\n",
        "def best_model_metrics(best_model):\n",
        "  global model_type, eval_dic\n",
        "  best_model.fit(st.session_state['X_train'], st.session_state['y_train'])\n",
        "\n",
        "  predict_train = best_model.predict(st.session_state['X_train'])\n",
        "  predict_test = best_model.predict(st.session_state['X_test'])\n",
        "  eval_dic = {}\n",
        "  if st.session_state['model_type'] == 'classification':\n",
        "    eval_dic[\"train\"] = {}\n",
        "    eval_dic[\"train\"][\"accuracy\"] = metrics.accuracy_score(st.session_state['y_train'], predict_train)\n",
        "    eval_dic[\"train\"][\"precision\"] = metrics.precision_score(st.session_state['y_train'], predict_train)\n",
        "    eval_dic[\"train\"][\"recall\"] = metrics.recall_score(st.session_state['y_train'], predict_train)\n",
        "    eval_dic[\"train\"][\"f1\"] = metrics.f1_score(st.session_state['y_train'], predict_train)\n",
        "    eval_dic[\"test\"] = {}\n",
        "    eval_dic[\"test\"][\"accuracy\"] = metrics.accuracy_score(st.session_state['y_test'], predict_test)\n",
        "    eval_dic[\"test\"][\"precision\"] = metrics.precision_score(st.session_state['y_test'], predict_test)\n",
        "    eval_dic[\"test\"][\"recall\"] = metrics.recall_score(st.session_state['y_test'], predict_test)\n",
        "    eval_dic[\"test\"][\"f1\"] = metrics.f1_score(st.session_state['y_test'], predict_test)\n",
        "  elif st.session_state['model_type'] == 'regression':\n",
        "    eval_dic[\"train\"] = {}\n",
        "    eval_dic[\"train\"][\"MAE\"] = mean_absolute_error(st.session_state['y_train'], predict_train)\n",
        "    eval_dic[\"train\"][\"MSE\"] = mean_squared_error(st.session_state['y_train'], predict_train)\n",
        "    eval_dic[\"train\"][\"RMSE\"] = (np.sqrt(mean_squared_error(st.session_state['y_train'], predict_train)))\n",
        "    eval_dic[\"train\"][\"r2\"] = r2_score(st.session_state['y_train'], predict_train)\n",
        "    eval_dic[\"test\"] = {}\n",
        "    eval_dic[\"test\"][\"MAE\"] = mean_absolute_error(st.session_state['y_test'], predict_test)\n",
        "    eval_dic[\"test\"][\"MSE\"] = mean_squared_error(st.session_state['y_test'], predict_test)\n",
        "    eval_dic[\"test\"][\"RMSE\"] = (np.sqrt(mean_squared_error(st.session_state['y_test'], predict_test)))\n",
        "    eval_dic[\"test\"][\"r2\"] = r2_score(st.session_state['y_test'], predict_test)\n",
        "  return eval_dic\n",
        "\n",
        "def extract_list(text):\n",
        "  # Extract all numeric values from the string\n",
        "  numbers = re.findall(r'\\d+', text)\n",
        "\n",
        "  # Convert the extracted numbers to integers\n",
        "  number_list = [int(num) for num in numbers]\n",
        "  return number_list\n",
        "\n",
        "if 'class_plots' not in st.session_state:\n",
        "    st.session_state['class_plots'] = ['Bland-Altman Plot', 'Learning Curves plot', 'Residuals Plot', 'ROC Curve', 'Precision-Recall Curve', 'Feature Importance', 'Confusion Matrix']\n",
        "if 'reg_plots' not in st.session_state:\n",
        "    st.session_state['reg_plots'] = ['Residuals Plot', 'Predictions vs Actual', 'Learning Curves plot']\n",
        "if 'X' not in st.session_state:\n",
        "    st.session_state[''] = None\n",
        "if 'y' not in st.session_state:\n",
        "    st.session_state['y'] = None\n",
        "if 'X_train_processed' not in st.session_state:\n",
        "    st.session_state['X_train_processed'] = None\n",
        "if 'y_train_processed' not in st.session_state:\n",
        "    st.session_state['y_train_processed'] = None\n",
        "if 'X_test_processed' not in st.session_state:\n",
        "    st.session_state['X_test_processed'] = None\n",
        "if 'y_train_processed' not in st.session_state:\n",
        "    st.session_state['y_train_processed'] = None\n",
        "if 'X_train' not in st.session_state:\n",
        "    st.session_state['X_train'] = None\n",
        "if 'y_train' not in st.session_state:\n",
        "    st.session_state['y_train'] = None\n",
        "if 'X_test' not in st.session_state:\n",
        "    st.session_state['X_test'] = None\n",
        "if 'y_test' not in st.session_state:\n",
        "    st.session_state['y_test'] = None\n",
        "if 'models_dic' not in st.session_state:\n",
        "    st.session_state['models_dic'] = {}\n",
        "if 'params_dic' not in st.session_state:\n",
        "    st.session_state['params_dic'] = {}\n",
        "if 'eval_method' not in st.session_state:\n",
        "    st.session_state['eval_method'] = None\n",
        "if 'preprocessor' not in st.session_state:\n",
        "    st.session_state['preprocessor'] = None\n",
        "if 'models_extracted' not in st.session_state:\n",
        "    st.session_state['models_extracted'] = []\n",
        "if 'prediction_value' not in st.session_state:\n",
        "    st.session_state['prediction_value'] = None\n",
        "if 'num_of_models' not in st.session_state:\n",
        "    st.session_state['num_of_models'] = None\n",
        "if 'num_of_models' not in st.session_state:\n",
        "    st.session_state['num_of_models'] = None\n",
        "if 'pred_val_set' not in st.session_state:\n",
        "    st.session_state['pred_val_set'] = None\n",
        "if 'df_columns' not in st.session_state:\n",
        "    st.session_state['df_columns'] = None\n",
        "if 'dataset_pred' not in st.session_state:\n",
        "    st.session_state['dataset_pred'] = None\n",
        "def full_analysis():\n",
        "  df_col_names_str = \", \".join(st.session_state['df'].columns)\n",
        "  input_user = str(st.session_state['learning_question'])+\" columns names: \"+df_col_names_str+\". what column name is the prediction value? pick the right column name and return the column name only as answer\"\n",
        "  gpt_response = call_gpt(input_user)\n",
        "  if gpt_response in st.session_state['df'].columns:\n",
        "    st.session_state['prediction_value'] = gpt_response\n",
        "  else:\n",
        "    st.session_state['prediction_value'] = st.selectbox(\n",
        "    label=r\"$\\textsf{\\Large Choose prediction column?}$\",\n",
        "    options=df_col_names_str.split(\",\"),  # Split the string into options\n",
        "    index=None,\n",
        "    placeholder=\"Select contact method...\",\n",
        "    )\n",
        "  if st.session_state['prediction_value']:\n",
        "    print(\"prediction_value = \", st.session_state['prediction_value'], \" type = \", type(st.session_state['prediction_value']))\n",
        "\n",
        "    st.session_state['df_columns'] = list(st.session_state['df'].columns.drop(st.session_state['prediction_value']))\n",
        "\n",
        "    st.session_state['y'] = st.session_state['df'][st.session_state['prediction_value']]\n",
        "\n",
        "    feature_columns = [col for col in st.session_state['df'].columns if col != st.session_state['prediction_value']]\n",
        "    st.session_state['X'] = st.session_state['df'].loc[:, feature_columns]\n",
        "    st.session_state['X_train'], st.session_state['X_test'], st.session_state['y_train'], st.session_state['y_test'] = train_test_split(st.session_state['X'], st.session_state['y'],\n",
        "                                        test_size=0.8,\n",
        "                                        random_state=1)\n",
        "\n",
        "\n",
        "    st.session_state['model_type'] = classification_or_regression(st.session_state['prediction_value'])\n",
        "    print(\"model_type = \", st.session_state['model_type'])\n",
        "\n",
        "    ques = \"return the content of the learning question \"+st.session_state['learning_question']+\" and the dataset columns \"+  df_col_names_str+\" and the prediction value \"+st.session_state['prediction_value']+\" for example breast cancer, housing price. dont add text\"\n",
        "    ans = call_gpt(ques)\n",
        "    st.session_state['dataset_pred'] = ans\n",
        "    print(\"check: st.session_state['dataset_pred'] = \", st.session_state['dataset_pred'])\n",
        "\n",
        "    if st.session_state['model_type'] == 'regression':\n",
        "      reg_plots_str = ' '.join(st.session_state['reg_plots'])\n",
        "      ques = \"pick from the following list \"+reg_plots_str+\" the right plot to answer the learning question \"+st.session_state['learning_question']+\" if nothing right return other. dont add text.\"\n",
        "      ans = call_gpt(ques)\n",
        "      st.session_state['reg_plots'] = ans\n",
        "      print(\"check st.session_state['reg_plots'] = \", st.session_state['reg_plots'])\n",
        "    elif st.session_state['model_type'] == 'classification':\n",
        "      class_plots_str = ' '.join(st.session_state['class_plots'])\n",
        "      ques = \"pick from the following list \"+class_plots_str+\" the right plot to answer the learning question \"+st.session_state['learning_question']+\" if nothing right return other. dont add text.\"\n",
        "      ans = call_gpt(ques)\n",
        "      st.session_state['class_plots'] = ans\n",
        "      print(\"check st.session_state['class_plots'] = \", st.session_state['class_plots'])\n",
        "\n",
        "      if st.session_state['encode_target'] == False:\n",
        "        st.session_state['pred_val_set'] = set(st.session_state['y'])\n",
        "        pred_set_lst = list(st.session_state['pred_val_set'])\n",
        "        pred_set_str = [str(i) for i in pred_set_lst]\n",
        "        pred_str = ' '.join(list(pred_set_str))\n",
        "        file_id = st.session_state['file_id']\n",
        "        lr_ques = st.session_state['learning_question']\n",
        "        ques = \"from \"+lr_ques+\" and the dataset with file ID \"+str(file_id)+\" and the prediction values set \"+pred_str+\" return only dictionary that the keys are \"+pred_str+\" and the values are their meanings. dont add text\"\n",
        "        print(\"ques: \", ques)\n",
        "        ans = call_gpt(ques)\n",
        "        st.session_state['pred_val_dic'] = ast.literal_eval(ans)\n",
        "        print(\"check: \", ans, \" dic: \", st.session_state['pred_val_dic'], \" type = \", type(st.session_state['pred_val_dic']))\n",
        "\n",
        "    st.session_state['models_dic'], st.session_state['params_dic'], st.session_state['eval_method'] = grid_search_parameters(st.session_state['model_type'], st.session_state['models_dic'], st.session_state['params_dic'], st.session_state['eval_method'])\n",
        "\n",
        "    st.session_state['preprocessor'] = preprocessing(st.session_state['model_type'], st.session_state['preprocessor'], st.session_state['prediction_value'])\n",
        "\n",
        "    ques = \"Given the database type is \"+str(st.session_state['model_type'])+\"  regarding the dataset with file ID {st.session_state['file_id']}\"+\" give me in descending order the machine models with the highest probability of optimal performance on this data set in a list seperated by commas without additional text. \"\n",
        "\n",
        "    ans = call_gpt(ques)\n",
        "    st.session_state['models_extracted'] = list(ans.split(\", \"))\n",
        "\n",
        "    for model in list(st.session_state['models_dic'].keys()):\n",
        "      if model not in st.session_state['models_extracted']:\n",
        "        st.session_state['models_extracted'].append(model)\n",
        "\n",
        "    st.session_state['num_of_models'] = len(st.session_state['models_extracted'])\n",
        "\n",
        "def start_full_analysis():\n",
        "   st.session_state['output'] = full_grid_search()\n",
        "   return st.session_state['output']\n",
        "\n",
        "def map_model_name(model_name, model_type):\n",
        "    name_mapper = {\n",
        "        'Support Vector Machines': 'Support Vector Classifier' if model_type == 'classification' else 'Support Vector Regressor',\n",
        "        'Random Forest': 'Random Forest Classifier' if model_type == 'classification' else 'Random Forest Regressor',\n",
        "        'Gradient Boosting': 'Gradient Boosting Classifier' if model_type == 'classification' else 'Gradient Boosting Regressor'\n",
        "    }\n",
        "    if model_name in list(name_mapper.keys()):\n",
        "      return name_mapper.get(model_name)\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "def run_grid_search(pipe, param_grid, model_name, X_train, y_train):\n",
        "    clf = GridSearchCV(estimator=pipe, param_grid=param_grid[model_name], cv=5, error_score='raise')\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    return clf.best_estimator_, clf.best_score_, clf.best_params_, clf\n",
        "\n",
        "\n",
        "def check_model_metrics(best_model_name, metrics_dic):\n",
        "    if st.session_state['model_type'] == 'classification':\n",
        "      ques = \"these the model evaluation metrics train: accuracy= \"+str(round(metrics_dic[\"train\"][\"accuracy\"],3))+\", precision= \"+str(round(metrics_dic[\"train\"][\"precision\"],3))+\", recall= \"+str(round(metrics_dic[\"train\"][\"recall\"],3))+\", f1= \"+str(round(metrics_dic[\"train\"][\"f1\"],3))+\". test: accuracy= \"+str(round(metrics_dic[\"test\"][\"accuracy\"],3))+\", precision= \"+str(round(metrics_dic[\"test\"][\"precision\"],3))+\", recall= \"+str(round(metrics_dic[\"test\"][\"recall\"],3))+\", f1=\"+str(round(metrics_dic[\"test\"][\"f1\"],3))\n",
        "    elif st.session_state['model_type'] == 'regression':\n",
        "      ques = \"these the model evaluation metrics train: mean absolute error= \"+str(round(metrics_dic[\"train\"][\"MAE\"],3))+\", mean squared error= \"+str(round(metrics_dic[\"train\"][\"MSE\"],3))+\", root mean squared error= \"+str(round(metrics_dic[\"train\"][\"RMSE\"],3))+\", r2=\"+str(round(metrics_dic[\"train\"][\"r2\"],3))+\" test: mean absolute error= \"+str(round(metrics_dic[\"test\"][\"MAE\"],3))+\", mean squared error= \"+str(round(metrics_dic[\"test\"][\"MSE\"],3))+\", root mean squared error= \"+str(round(metrics_dic[\"test\"][\"RMSE\"],3))+\", r2= \"+str(round(metrics_dic[\"test\"][\"r2\"],3))\n",
        "    if st.session_state['pred_val_dic'] is not None:\n",
        "      ques += \" Could you explain in 4 lines and interpret the model's performance for a layperson on the datset in file ID \"+str(st.session_state['file_id'])+\"? use prediction dictionary \"+str(st.session_state['pred_val_dic'])+\" use learning question content: \"+st.session_state['dataset_pred']\n",
        "    else:\n",
        "      ques += \" Could you explain in 4 lines and interpret the model's performance for a layperson on the datset in file ID \"+str(st.session_state['file_id'])+\" ? use learning question content: \"+st.session_state['dataset_pred']\n",
        "    print(\"ques: \", ques)\n",
        "    ans = call_gpt(ques)\n",
        "    print(ans)\n",
        "    return ans\n",
        "\n",
        "def handle_edge_case(param_grid, model_name, clf, call_gpt):\n",
        "    repeat = False\n",
        "    for key, val in clf.best_params_.items():\n",
        "        current_grid = param_grid[model_name][key]\n",
        "        if isinstance(current_grid[0], (int, float)) and not isinstance(current_grid[0], bool):\n",
        "          if param_grid[model_name][key][0] == val or param_grid[model_name][key][-1] == val:\n",
        "            repeat = True\n",
        "\n",
        "            # Building a more detailed query\n",
        "            if val == current_grid[-1]:  # Best value is the highest in the range\n",
        "                direction = \"higher\"\n",
        "                extreme = 'highest'\n",
        "            elif val == current_grid[0]:  # Best value is the lowest in the range\n",
        "                direction = \"lower\"\n",
        "                extreme = 'lowest'\n",
        "            param_name = key.split('__')[-1]\n",
        "            min_limit = None\n",
        "            max_limit = None\n",
        "            model_limits_dic = get_param_limits(model_name)\n",
        "            if len(model_limits_dic) > 0:\n",
        "              param_limit_dic = model_limits_dic.get(param_name)\n",
        "              if param_limit_dic is not None:\n",
        "                min_limit = param_limit_dic.get('min')\n",
        "                max_limit = param_limit_dic.get('max')\n",
        "\n",
        "            ques =f\"I am optimizing a machine learning model using GridSearchCV. For the model '{model_name}' and the hyperparameter '{param_name}', I previously tried the values {current_grid} and found that the best performing value was {val}. I want to explore {direction} values beyond {val} to find the {extreme} value that still improves model performance, It's crucial that the new values strictly adhere to the range between {min_limit} and {max_limit}. Can you suggest a new list of values for '{param_name}' to try? The list should include {val} as a reference point, not exceed. arrange the list in ascending order.\"\n",
        "            ans = call_gpt(ques)\n",
        "            new_lst = extract_list(ans)\n",
        "            new_lst = sorted(set(num for num in new_lst if num != 0))\n",
        "            if len(new_lst) > 1 :\n",
        "              if new_lst[0] == val:\n",
        "                new_lst = new_lst[1:]\n",
        "            else:\n",
        "              repeat = False\n",
        "            param_grid[model_name][key] = new_lst\n",
        "    return repeat, param_grid\n",
        "\n",
        "\n",
        "if 'model_num' not in st.session_state:\n",
        "    st.session_state['model_num'] = 0\n",
        "if 'best_models' not in st.session_state:\n",
        "    st.session_state['best_models'] = {}\n",
        "if 'best_scores' not in st.session_state:\n",
        "    st.session_state['best_scores'] = {}\n",
        "if 'best_params' not in st.session_state:\n",
        "    st.session_state['best_params'] = {}\n",
        "if 'best_model_name' not in st.session_state:\n",
        "    st.session_state['best_model_name'] = ''\n",
        "if 'best_score' not in st.session_state:\n",
        "    st.session_state['best_score'] = 0\n",
        "if 'best_model_params' not in st.session_state:\n",
        "    st.session_state['best_model_params'] = None\n",
        "if 'output' not in st.session_state:\n",
        "    st.session_state['output'] = None\n",
        "if 'metrics_dic' not in st.session_state:\n",
        "    st.session_state['metrics_dic'] = {}\n",
        "if 'metric_explanation' not in st.session_state:\n",
        "    st.session_state['metric_explanation'] = None\n",
        "if 'pipe' not in st.session_state:\n",
        "    st.session_state['pipe'] = None\n",
        "if 'best_pipe' not in st.session_state:\n",
        "    st.session_state['best_pipe'] = None\n",
        "if 'models' not in st.session_state:\n",
        "    st.session_state['models'] = None\n",
        "if 'iteration_num' not in st.session_state:\n",
        "    st.session_state['iteration_num'] = None\n",
        "if 'fast_analize' not in st.session_state:\n",
        "    st.session_state['fast_analize'] = False\n",
        "if 'option' not in st.session_state:\n",
        "    st.session_state['option'] = None\n",
        "if 'analyze_button' not in st.session_state:\n",
        "    st.session_state['analyze_button'] = False\n",
        "\n",
        "def get_param_limits(model_name):\n",
        "    if model_name == 'Random Forest Regressor':\n",
        "        return {\n",
        "            'n_estimators': {'min': 10, 'max': None},\n",
        "            'max_depth': {'min': 1, 'max': None},\n",
        "            'min_samples_split': {'min': 2, 'max': None}\n",
        "        }\n",
        "    elif model_name == 'Gradient Boosting Regressor':\n",
        "        return {\n",
        "            'learning_rate': {'min': 0.001, 'max': 1},\n",
        "            'min_samples_split': {'min': 2, 'max': None}\n",
        "        }\n",
        "    elif model_name == 'Support Vector Regressor':\n",
        "        return {\n",
        "            'C': {'min': 0.001, 'max': None},\n",
        "            'gamma': {'min': 0.001, 'max': 1}\n",
        "        }\n",
        "    elif model_name == 'Linear Regression':\n",
        "        return {\n",
        "            # Linear Regression usually does not need hyperparameter tuning\n",
        "        }\n",
        "    elif model_name == 'Ridge Regression':\n",
        "        return {\n",
        "            'alpha': {'min': 0.001, 'max': None},\n",
        "            'max_iter': {'min': 1, 'max': None}\n",
        "        }\n",
        "    elif model_name == 'Lasso Regression':\n",
        "        return {\n",
        "            'alpha': {'min': 0.001, 'max': None},\n",
        "            'max_iter': {'min': 1, 'max': None}\n",
        "        }\n",
        "    elif model_name == 'ElasticNet Regression':\n",
        "        return {\n",
        "            'alpha': {'min': 0.001, 'max': None},\n",
        "            'l1_ratio': {'min': 0, 'max': 1}\n",
        "        }\n",
        "    elif model_name == 'Decision tree Regressor':\n",
        "        return {\n",
        "            'max_depth': {'min': 1, 'max': None},\n",
        "            'min_samples_split': {'min': 2, 'max': None},\n",
        "            'min_samples_leaf': {'min': 1, 'max': None}\n",
        "        }\n",
        "    elif model_name == 'Random Forest Classifier':\n",
        "        return {\n",
        "            'n_estimators': {'min': 10, 'max': None},\n",
        "            'max_depth': {'min': 1, 'max': None},\n",
        "            'min_samples_split': {'min': 2, 'max': None}\n",
        "        }\n",
        "    elif model_name == 'Logistic Regression':\n",
        "        return {\n",
        "            'C': {'min': 0.001, 'max': None}\n",
        "        }\n",
        "    elif model_name == 'Support Vector Classifier':\n",
        "        return {\n",
        "            'C': {'min': 0.001, 'max': None},\n",
        "            'gamma': {'min': 0.001, 'max': 1}\n",
        "        }\n",
        "    elif model_name == 'K-Nearest Neighbors':\n",
        "        return {\n",
        "            'n_neighbors': {'min': 1, 'max': None}\n",
        "        }\n",
        "    elif model_name == 'Gradient Boosting Classifier':\n",
        "        return {\n",
        "            'learning_rate': {'min': 0.001, 'max': 1},\n",
        "            'max_depth': {'min': 1, 'max': None}\n",
        "        }\n",
        "    elif model_name == 'XGB Classifier':\n",
        "        return {\n",
        "            'n_estimators': {'min': 10, 'max': None},\n",
        "            'max_depth': {'min': 1, 'max': None},\n",
        "            'min_child_weight': {'min': 1, 'max': None}\n",
        "        }\n",
        "    elif model_name == 'Decision Tree Classifier':\n",
        "        return {\n",
        "            'max_depth': {'min': 1, 'max': None},\n",
        "            'min_samples_split': {'min': 2, 'max': None},\n",
        "            'min_samples_leaf': {'min': 1, 'max': None}\n",
        "        }\n",
        "    elif model_name == 'Naive Bayes':\n",
        "        return {}  # Naive Bayes usually doesn't need hyperparameter tuning\n",
        "\n",
        "    return {}\n",
        "\n",
        "def full_grid_search():\n",
        "  param_grid = st.session_state['params_dic']\n",
        "\n",
        "  if st.session_state['models'] is not None:\n",
        "      models_lst = list(st.session_state['models'].keys())\n",
        "\n",
        "      for model_name in models_lst:\n",
        "          new_model_name = map_model_name(model_name, st.session_state['model_type'])\n",
        "\n",
        "          if new_model_name is not None:\n",
        "              model_name = new_model_name\n",
        "\n",
        "          i, repeat = 0, True\n",
        "          while i < st.session_state['iteration_num'] and repeat:\n",
        "              repeat = False\n",
        "              pipe_steps = [('preprocessor', st.session_state['preprocessor']), ('model', st.session_state['models'][model_name])]\n",
        "              st.session_state['pipe'] = Pipeline(pipe_steps)\n",
        "\n",
        "              best_model_curr, best_score_curr, best_params_curr, clf = run_grid_search(\n",
        "                  st.session_state['pipe'], param_grid, model_name, st.session_state['X_train_processed'], st.session_state['y_train'])\n",
        "\n",
        "              st.session_state['best_models'][model_name] = best_model_curr\n",
        "              st.session_state['best_scores'][model_name] = best_score_curr\n",
        "              st.session_state['best_params'][model_name] = best_params_curr\n",
        "\n",
        "              repeat, param_grid = handle_edge_case(param_grid, model_name, clf, call_gpt)\n",
        "              i += 1\n",
        "\n",
        "          if best_score_curr > st.session_state['best_score']:\n",
        "              st.session_state['best_score'] = round(best_score_curr,3)\n",
        "              st.session_state['best_model'] = best_model_curr\n",
        "              st.session_state['best_model_name'] = model_name\n",
        "              st.session_state['best_model_params'] = best_params_curr\n",
        "              st.session_state['best_pipe'] = st.session_state['pipe']\n",
        "\n",
        "              st.session_state['metrics_dic'] = best_model_metrics(st.session_state['best_model'])\n",
        "              st.session_state['metric_explanation'] = check_model_metrics(st.session_state['best_model_name'], st.session_state['metrics_dic'])\n",
        "\n",
        "              st.session_state['output'] = f\"The best model found so far is {st.session_state['best_model_name']} with the score: {st.session_state['best_score']}. \\nexplanation:\\n {st.session_state['metric_explanation']}.\"\n",
        "\n",
        "  return st.session_state['output']\n",
        "\n",
        "def aggregate_importances(importances, feature_names, original_cat_features):\n",
        "    \"\"\"\n",
        "    Aggregate the importances of one-hot encoded features back to the original feature.\n",
        "\n",
        "    :param importances: Array of importances from the model.\n",
        "    :param feature_names: List of all features names after OneHotEncoder transformation.\n",
        "    :param original_cat_features: List of original categorical feature names before encoding.\n",
        "    :return: Dictionary of aggregated feature importances.\n",
        "    \"\"\"\n",
        "    aggregated_importances = {}\n",
        "    feature_names = list(feature_names)  # Convert to list if it's not already\n",
        "    print(\"check original_cat_features = \", original_cat_features)\n",
        "    print(\"check feature_names = \", feature_names)\n",
        "    for original_feature in original_cat_features:\n",
        "        # Sum the importances of features that originally belonged to the same categorical feature\n",
        "        aggregated_importances[original_feature] = sum(importances[i] for i, feature in enumerate(feature_names) if feature.startswith(original_feature))\n",
        "        print(\"check original_feature = \", original_feature, \" aggregated_importances[original_feature] = \", aggregated_importances[original_feature])\n",
        "    # Add importances of non-categorical features\n",
        "    for feature in feature_names:\n",
        "        if feature.split('_')[0] not in original_cat_features:\n",
        "            aggregated_importances[feature] = importances[feature_names.index(feature)]\n",
        "            #feature_index = np.where(np.array(feature_names) == feature)[0][0]  # Find the index of the feature\n",
        "            print(\"check feature = \", feature, \" aggregated_importances[feature] = \", aggregated_importances[feature])\n",
        "    print(\"check aggregated_importances = \", aggregated_importances)\n",
        "    return aggregated_importances\n",
        "\n",
        "\n",
        "def display_results():\n",
        "  y_pred = st.session_state['best_model'].predict(st.session_state['X_test'])\n",
        "\n",
        "  st.header(\"Model Results\")\n",
        "  model = st.session_state['best_pipe'].named_steps['model']\n",
        "\n",
        "  print(\"check st.session_state['reg_plots'] = \", st.session_state['reg_plots'], \" st.session_state['class_plots'] = \", st.session_state['class_plots'])\n",
        "  if st.session_state['class_plots'] == 'Learning Curves plot' or st.session_state['reg_plots'] == 'Learning Curves plot':\n",
        "      # Learning Curves\n",
        "      train_sizes, train_scores, test_scores = learning_curve(st.session_state['best_model'], st.session_state['X_train_processed'], st.session_state['y_train'], cv=5)\n",
        "      train_scores_mean = np.mean(train_scores, axis=1)\n",
        "      test_scores_mean = np.mean(test_scores, axis=1)\n",
        "      fig = plt.figure(figsize=(10, 6))\n",
        "      plt.plot(train_sizes, train_scores_mean, label='Training score')\n",
        "      plt.plot(train_sizes, test_scores_mean, label='Cross-validation score')\n",
        "      plt.xlabel('Training Size')\n",
        "      plt.ylabel('Score')\n",
        "      plt.title('Learning Curves')\n",
        "      plt.legend()\n",
        "      st.pyplot(fig)\n",
        "      get_plt_explanation(\"explain the plot for a layperson regarding the analysis on the datset in file ID \"+str(st.session_state['file_id'])+\" and the \"+st.session_state['learning_question']+\" and conclusion base on the best model found \"+st.session_state['best_model_name']+\" use prediction values: \"+str(st.session_state['pred_val_dic'])+\" answer the learning question: \"+ str(st.session_state['learning_question']))\n",
        "\n",
        "  if st.session_state['model_type'] == 'classification':\n",
        "    if st.session_state['class_plots'] == 'Bland-Altman Plot':\n",
        "      # Bland-Altman Plot\n",
        "      mean = np.mean([st.session_state['y_test'], y_pred], axis=0)\n",
        "      diff = st.session_state['y_test'] - y_pred\n",
        "      fig = plt.figure(figsize=(10, 6))\n",
        "      plt.scatter(mean, diff)\n",
        "      plt.axhline(np.mean(diff), color='gray', linestyle='--')\n",
        "      plt.axhline(np.mean(diff) + 1.96*np.std(diff), color='gray', linestyle='--')\n",
        "      plt.axhline(np.mean(diff) - 1.96*np.std(diff), color='gray', linestyle='--')\n",
        "      plt.xlabel('Mean of True and Predicted')\n",
        "      plt.ylabel('Difference')\n",
        "      plt.title('Bland-Altman Plot')\n",
        "      st.pyplot(fig)\n",
        "      get_plt_explanation(\"explain the Bland-Altman plot for a layperson regarding the analysis on the datset in file ID \"+str(st.session_state['file_id'])+\" and the \"+st.session_state['learning_question']+\" and conclusion base on the best model found \"+st.session_state['best_model_name']+\" use prediction values: \"+str(st.session_state['pred_val_dic'])+\" answer the learning question: \"+ str(st.session_state['learning_question']))\n",
        "    elif st.session_state['class_plots'] == 'ROC Curve':\n",
        "      if len(set(st.session_state['y_test'])) == 2:\n",
        "        fpr, tpr, _ = roc_curve(st.session_state['y_test'], y_pred)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        # Create a Matplotlib figure\n",
        "        fig, ax = plt.subplots()\n",
        "        # Plot the ROC curve\n",
        "        ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:0.2f})')\n",
        "        ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "        ax.set_xlim([0.0, 1.0])\n",
        "        ax.set_ylim([0.0, 1.05])\n",
        "        ax.set_xlabel('False Positive Rate', fontsize=13)\n",
        "        ax.set_ylabel('True Positive Rate', fontsize=13)\n",
        "        ax.set_title('Receiver Operating Characteristic', fontsize=17)\n",
        "        ax.legend(loc=\"lower right\")\n",
        "        # Display the figure using Streamlit\n",
        "        st.header('ROC Curve')\n",
        "        st.pyplot(fig)\n",
        "        get_plt_explanation(\"explain the ROC Curve plot for a layperson regarding the analysis on the datset in file ID \"+str(st.session_state['file_id'])+\" and the \"+st.session_state['learning_question']+\" and conclusion base on the best model found \"+st.session_state['best_model_name']+\" use prediction values: \"+str(st.session_state['pred_val_dic'])+\" answer the learning question: \"+ str(st.session_state['learning_question']))\n",
        "      elif len(set(st.session_state['y_test'])) > 2:\n",
        "        # Binarize the output labels for multi-class\n",
        "        y = label_binarize(st.session_state['y_test'], classes=np.unique(st.session_state['y_test']))\n",
        "        n_classes = y.shape[1]\n",
        "\n",
        "        # Assuming 'model' is your trained multi-class classifier\n",
        "        # You may need to use OneVsRestClassifier in training\n",
        "        y_scores = model.predict_proba(st.session_state['X_test'])\n",
        "\n",
        "        # Compute ROC curve and ROC area for each class\n",
        "        fpr = dict()\n",
        "        tpr = dict()\n",
        "        roc_auc = dict()\n",
        "        for i in range(n_classes):\n",
        "            fpr[i], tpr[i], _ = roc_curve(y[:, i], y_scores[:, i])\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "        # Plotting all ROC curves\n",
        "        plt.figure()\n",
        "        colors = cycle(['blue', 'red', 'green', 'yellow', 'cyan'])\n",
        "        for i, color in zip(range(n_classes), colors):\n",
        "            plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "                    label=f'ROC curve of class {i} (area = {roc_auc[i]:.2f})')\n",
        "\n",
        "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Receiver Operating Characteristic for multi-class')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        st.pyplot(plt)\n",
        "        get_plt_explanation(\"explain the ROC Curve plot for a layperson regarding the analysis on the datset in file ID \"+str(st.session_state['file_id'])+\" and the \"+st.session_state['learning_question']+\" and conclusion base on the best model found \"+st.session_state['best_model_name']+\" use prediction values: \"+str(st.session_state['pred_val_dic'])+\" answer the learning question: \"+ str(st.session_state['learning_question']))\n",
        "    elif st.session_state['class_plots'] == 'Precision-Recall Curve':\n",
        "      if len(set(st.session_state['y_test'])) == 2:\n",
        "        from sklearn.metrics import precision_recall_curve\n",
        "        precision, recall, _ = precision_recall_curve(st.session_state['y_test'], y_scores)\n",
        "\n",
        "        plt.figure()\n",
        "        plt.step(recall, precision, where='post')\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.title('Precision-Recall Curve')\n",
        "        st.pyplot(plt)\n",
        "        get_plt_explanation(\"explain the Precision-Recall Curve plot for a layperson regarding the analysis on the datset in file ID \"+str(st.session_state['file_id'])+\" and the \"+st.session_state['learning_question']+\" and conclusion base on the best model found \"+st.session_state['best_model_name']+\" use prediction values: \"+str(st.session_state['pred_val_dic']))\n",
        "      elif len(set(st.session_state['y_test'])) > 2:\n",
        "        from sklearn.preprocessing import label_binarize\n",
        "        from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "        # Assuming 'y_true' are your true multi-class labels and 'n_classes' is the number of classes\n",
        "        y_true_binarized = label_binarize(st.session_state['y_test'], classes=np.arange(n_classes))\n",
        "\n",
        "        # 'y_scores' should have shape [n_samples, n_classes]\n",
        "        for i in range(n_classes):\n",
        "            precision, recall, _ = precision_recall_curve(y_true_binarized[:, i], y_scores[:, i])\n",
        "\n",
        "            plt.figure()\n",
        "            plt.step(recall, precision, where='post')\n",
        "            plt.xlabel('Recall')\n",
        "            plt.ylabel('Precision')\n",
        "            plt.title(f'Precision-Recall Curve for Class {i}')\n",
        "            st.pyplot(plt)\n",
        "            get_plt_explanation(\"explain the Precision-Recall Curve plot for a layperson regarding the analysis on the datset in file ID \"+str(st.session_state['file_id'])+\" and the \"+st.session_state['learning_question']+\" and conclusion base on the best model found \"+st.session_state['best_model_name']+\" use prediction values: \"+str(st.session_state['pred_val_dic'])+\" answer the learning question: \"+ str(st.session_state['learning_question']))\n",
        "\n",
        "    elif st.session_state['class_plots'] == 'Residuals Plot':\n",
        "      # Residuals Plot\n",
        "      residuals = st.session_state['y_test'] - y_pred\n",
        "      fig = plt.figure(figsize=(10, 6))\n",
        "      plt.scatter(y_pred, residuals)\n",
        "      plt.hlines(y=0, xmin=y_pred.min(), xmax=y_pred.max(), colors='red')\n",
        "      plt.xlabel('Predicted')\n",
        "      plt.ylabel('Residuals')\n",
        "      plt.title('Residuals Plot')\n",
        "      #st.pyplot(plt.show())\n",
        "      st.pyplot(fig)\n",
        "      get_plt_explanation(\"explain the Residuals Plot plot for a layperson regarding the analysis on the datset in file ID \"+str(st.session_state['file_id'])+\" and the \"+st.session_state['learning_question']+\" and conclusion base on the best model found \"+st.session_state['best_model_name']+\" use prediction values: \"+str(st.session_state['pred_val_dic'])+\" answer the learning question: \"+ str(st.session_state['learning_question']))\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(st.session_state['y_test'], y_pred)\n",
        "\n",
        "    # Create a Matplotlib figure\n",
        "    fig, ax = plt.subplots()\n",
        "    # Plot the confusion matrix on the figure\n",
        "    cm_df = pd.DataFrame(cm, index = list(st.session_state['pred_val_dic'].values()), columns = list(st.session_state['pred_val_dic'].values()))\n",
        "    sns.heatmap(cm_df, annot=True, fmt='g', ax=ax)\n",
        "    #sns.heatmap(cm, annot=True, fmt='g', ax=ax)\n",
        "    plt.ylabel('Prediction', fontsize=13)\n",
        "    plt.xlabel('Actual', fontsize=13)\n",
        "    plt.title('Confusion Matrix', fontsize=17)\n",
        "\n",
        "    # Display the figure using Streamlit\n",
        "    st.header('Confusion Matrix')\n",
        "    st.pyplot(fig)\n",
        "    get_plt_explanation(\"explain the Confusion Matrix plot for a layperson regarding the analysis on the datset in file ID \"+str(st.session_state['file_id'])+\" and the \"+st.session_state['learning_question']+\" and conclusion base on the best model found \"+st.session_state['best_model_name']+\" use prediction values: \"+str(st.session_state['pred_val_dic']))\n",
        "\n",
        "    #feature importance\n",
        "    model = st.session_state['best_pipe'].named_steps['model']\n",
        "    if st.session_state['best_model_name'] != 'Support Vector Classifier':\n",
        "      #model.fit(st.session_state['X_train'], st.session_state['y_train'])\n",
        "      model.fit(st.session_state['X_train_processed'], st.session_state['y_train'])\n",
        "      #model = st.session_state['best_model']\n",
        "      importances = model.feature_importances_\n",
        "      indices = np.argsort(importances)[::-1]\n",
        "      fig = plt.figure(figsize=(10, 6))\n",
        "      #plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
        "      plt.barh(range(len(indices)), importances[indices], color='m', align='center')\n",
        "      plt.yticks(range(len(indices)), [st.session_state['X_train'].columns[i] for i in indices])\n",
        "      plt.xlabel('Relative Importance')\n",
        "\n",
        "      # Extract feature names\n",
        "      # Sort feature names and importances\n",
        "      feature_importance_pairs = list(zip(st.session_state['feature_names'], importances))\n",
        "      sorted_pairs = sorted(feature_importance_pairs, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "      st.pyplot(fig)\n",
        "      get_plt_explanation(\"explain the feature importance plot for a layperson regarding the analysis on the datset in file ID \"+str(st.session_state['file_id'])+\" and the \"+st.session_state['learning_question']+\" and conclusion base on the best model found \"+st.session_state['best_model_name']+\" use prediction values: \"+str(st.session_state['pred_val_dic']))\n",
        "    else:\n",
        "      if len(st.session_state['pred_val_set']) == 2:\n",
        "        # Fit the SVC model\n",
        "        model.fit(st.session_state['X_train_processed'], st.session_state['y_train'])\n",
        "        # Calculate permutation importance\n",
        "        perm_importances = permutation_importance(model, st.session_state['X_train_processed'], st.session_state['y_train'])\n",
        "        mean_perm_importances = np.mean(perm_importances['importances'], axis=0)\n",
        "        # Sort features by mean permutation importance in descending order\n",
        "        indices = np.argsort(mean_perm_importances)[::-1]\n",
        "\n",
        "        # Create a Matplotlib figure\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        # Plot the mean permutation importances\n",
        "        barlist = ax.barh(range(len(indices)), mean_perm_importances[indices], color='m', align='center')\n",
        "        plt.yticks(range(len(indices)), [st.session_state['X_train'].columns[i] for i in indices])\n",
        "        plt.xlabel('Mean Importance')\n",
        "        # Annotate the feature names with corresponding mean permutation importance values\n",
        "        for i, (label, bar) in enumerate(zip(ax.get_yticklabels(), barlist)):\n",
        "            ax.text(bar.get_width() + 0.01, i, label, ha='center', va='center')\n",
        "        plt.title('Average Feature Importance (Permutation Importance)')\n",
        "        plt.tight_layout()\n",
        "        # Display the figure using Streamlit\n",
        "        st.header('Feature Importance')\n",
        "        st.pyplot(fig)\n",
        "        get_plt_explanation(\"explain the feature importance plot for a layperson regarding the analysis on the datset in file ID \"+str(st.session_state['file_id'])+\" and the \"+st.session_state['learning_question']+\" and conclusion base on the best model found \"+st.session_state['best_model_name']+\" use prediction values: \"+str(st.session_state['pred_val_dic']))\n",
        "      elif len(st.session_state['pred_val_set']) > 2:\n",
        "        # Fit the SVC model\n",
        "        model.fit(st.session_state['X_train_processed'], st.session_state['y_train'])\n",
        "        # Calculate permutation importance\n",
        "        perm_importances = permutation_importance(model, st.session_state['X_train_processed'], st.session_state['y_train'])\n",
        "        mean_perm_importances = np.mean(perm_importances['importances'], axis=0)\n",
        "        # Sort features by mean permutation importance in descending order\n",
        "        indices = np.argsort(mean_perm_importances)[::-1]\n",
        "\n",
        "        # Create a Matplotlib figure\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        # Plot the mean permutation importances\n",
        "        barlist = ax.barh(range(len(indices)), mean_perm_importances[indices], color='m', align='center')\n",
        "        plt.yticks(range(len(indices)), [st.session_state['X_train'].columns[i] for i in indices])\n",
        "        plt.xlabel('Mean Importance')\n",
        "        # Annotate the feature names with corresponding mean permutation importance values\n",
        "        for i, (label, bar) in enumerate(zip(ax.get_yticklabels(), barlist)):\n",
        "            ax.text(bar.get_width() + 0.01, i, label, ha='center', va='center')\n",
        "        plt.title('Average Feature Importance (Permutation Importance)')\n",
        "        plt.tight_layout()\n",
        "        # Display the figure using Streamlit\n",
        "        st.header('Feature Importance')\n",
        "        st.pyplot(fig)\n",
        "        get_plt_explanation(\"explain the feature importance plot for a layperson regarding the analysis on the datset in file ID \"+str(st.session_state['file_id'])+\" and the \"+st.session_state['learning_question']+\" and conclusion base on the best model found \"+st.session_state['best_model_name']+\" use prediction values: \"+str(st.session_state['pred_val_dic']))\n",
        "\n",
        "  elif st.session_state['model_type'] == 'regression':\n",
        "    if st.session_state['reg_plots'] == 'Residuals Plot':\n",
        "       # Residual Plot\n",
        "      print(st.session_state['y_test'])#check\n",
        "      print(type(st.session_state['y_test']))#check\n",
        "      st.session_state['y_test'] = st.session_state['y_test'].to_frame()#check\n",
        "      residuals = st.session_state['y_test'][st.session_state['prediction_value']] - y_pred\n",
        "      fig = plt.figure(figsize=(10, 6))\n",
        "      plt.scatter(y_pred, residuals)\n",
        "      plt.hlines(y=0, xmin=y_pred.min(), xmax=y_pred.max(), colors='red')\n",
        "      plt.xlabel('Predicted')\n",
        "      plt.ylabel('Residuals')\n",
        "      plt.title('Residuals Plot')\n",
        "      st.pyplot(fig)\n",
        "      get_plt_explanation(\"explain the Residuals plot for a layperson regarding the analysis on the datset in file ID \"+str(st.session_state['file_id'])+\" and the \"+st.session_state['learning_question']+\" and conclusion base on the best model found \"+st.session_state['best_model_name']+\" use prediction values: \"+str(st.session_state['pred_val_dic'])+\" answer the learning question: \"+ str(st.session_state['learning_question']))\n",
        "\n",
        "       #Box Plot of Residuals\n",
        "      fig, ax = plt.subplots()\n",
        "      # Plot the boxplot of residuals using seaborn\n",
        "      sns.boxplot(residuals, ax=ax)\n",
        "      # Customize the plot\n",
        "      plt.xlabel('Residuals')\n",
        "      plt.title('Box Plot of Residuals')\n",
        "      # Display the plot using Streamlit\n",
        "      st.header('Distribution of Residuals')\n",
        "      st.pyplot(fig)\n",
        "      get_plt_explanation(\"explain the Box Plot of Residuals plot for a layperson regarding the analysis on the datset in file ID \"+str(st.session_state['file_id'])+\" and the \"+st.session_state['learning_question']+\" and conclusion base on the best model found \"+st.session_state['best_model_name']+\" use prediction values: \"+str(st.session_state['pred_val_dic']))\n",
        "\n",
        "      plt.figure()\n",
        "      sns.histplot(residuals, kde=True)\n",
        "      plt.xlabel('Residuals')\n",
        "      plt.title('Distribution of Residuals')\n",
        "      st.pyplot(plt)\n",
        "      get_plt_explanation(\"explain the Distribution of Residuals (Histogram) plot for a layperson regarding the analysis on the datset in file ID \"+str(st.session_state['file_id'])+\" and the \"+st.session_state['learning_question']+\" and conclusion base on the best model found \"+st.session_state['best_model_name']+\" use prediction values: \"+str(st.session_state['pred_val_dic'])+\" answer the learning question: \"+ str(st.session_state['learning_question']))\n",
        "\n",
        "    elif st.session_state['reg_plots'] == 'Predictions vs Actual':\n",
        "      # Predictions vs Actual\n",
        "      y_test = st.session_state['y_test'].to_frame()\n",
        "      prediction_value = st.session_state['prediction_value']\n",
        "      # Filter the 'y_test' data based on the selected prediction value\n",
        "      filtered_y_test = y_test[prediction_value]\n",
        "\n",
        "      # Create a DataFrame with the filtered 'y_test' and 'y_pred' data\n",
        "      df_results = pd.DataFrame({'Actual': filtered_y_test, 'Predicted': y_pred})\n",
        "      # Create the predictions vs actual values plot using Seaborn's lmplot\n",
        "      lm_fig = sns.lmplot(x='Actual', y='Predicted', data=df_results, fit_reg=False, aspect=1.2)\n",
        "      # Plot the diagonal line\n",
        "      d_line = np.arange(df_results.min().min(), df_results.max().max())\n",
        "      plt.plot(d_line, d_line, color='red', linestyle='--')\n",
        "      # Customize the plot labels and title\n",
        "      plt.xlabel('Actual '+ str(st.session_state['prediction_value']))\n",
        "      plt.ylabel('Predicted '+ str(st.session_state['prediction_value']))\n",
        "      plt.title('Predictions vs Actual')\n",
        "      # Display the plot using Streamlit\n",
        "      st.pyplot(lm_fig.fig)\n",
        "      get_plt_explanation(\"explain the Predictions vs Actual plot for a layperson regarding the analysis on the datset in file ID \"+str(st.session_state['file_id'])+\" and the \"+st.session_state['learning_question']+\" and conclusion base on the best model found \"+st.session_state['best_model_name']+\" use prediction values: \"+str(st.session_state['pred_val_dic'])+\" answer the learning question: \"+ str(st.session_state['learning_question']))\n",
        "\n",
        "    #feature importance\n",
        "    model = st.session_state['best_pipe'].named_steps['model']\n",
        "    model.fit(st.session_state['X_train_processed'], st.session_state['y_train'])\n",
        "    importances = model.feature_importances_\n",
        "    original_cat_features = [f for f in st.session_state['df_columns'].drop(st.session_state['prediction_value']) if classification_or_regression(f) == 'classification']\n",
        "    aggregated_importances = dict()\n",
        "    for orig_feature in original_cat_features:\n",
        "        # For each original feature, sum importances of all columns related to that feature\n",
        "        aggregated_importances[orig_feature] = sum(importances[[f.startswith(str(orig_feature)) for f in st.session_state['feature_names']]])\n",
        "\n",
        "    # Add non-categorical features' importances as-is\n",
        "    non_cat_features = [f for f in st.session_state['df_columns'].drop(st.session_state['prediction_value']) if classification_or_regression(f) == 'regression']\n",
        "\n",
        "    for f in non_cat_features:\n",
        "        aggregated_importances[f] = importances[list(st.session_state['feature_names']).index(str(f))]\n",
        "\n",
        "    # Convert to a list of (feature, importance) tuples and sort by importance\n",
        "    sorted_features = sorted(aggregated_importances.items(), key=lambda x: x[1], reverse=True)\n",
        "    # Split into separate lists for plotting\n",
        "    features, importances = zip(*sorted_features)\n",
        "    # Plot\n",
        "    plt.barh(features, importances, color='maroon')\n",
        "    plt.xlabel('Feature Importance')\n",
        "    plt.ylabel('Features')\n",
        "    plt.gca().invert_yaxis()  # To display the highest importance at the top\n",
        "    st.pyplot(plt)\n",
        "\n",
        "    get_plt_explanation(\"explain the feature importance plot for a layperson regarding the analysis on the datset in file ID \"+str(st.session_state['file_id'])+\" and the \"+st.session_state['learning_question']+\" and conclusion base on the best model found \"+st.session_state['best_model_name']+\" use prediction values: \"+str(st.session_state['pred_val_dic']))\n",
        "\n",
        "def get_plt_explanation(text):\n",
        "  buffer = BytesIO()\n",
        "  plt.savefig(buffer, format='png')\n",
        "  plt.close()\n",
        "  base64_image = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-4-vision-preview\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": text},\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "                    },\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=300,\n",
        ")\n",
        "  st.write(response.choices[0].message.content)\n",
        "\n",
        "def get_summary():\n",
        "\n",
        "  if st.session_state['pred_val_dic'] is not None:\n",
        "    ques = \"Summarize in 2 lines for a layperson the analysis on the dataset\"+\" and the learning question \"+st.session_state['learning_question']+\" and conclusion base on the best model found \"+st.session_state['best_model_name']+\" use prediction values: \"+str(st.session_state['pred_val_dic'])\n",
        "  else:\n",
        "    ques = \"Summarize in 2 lines for a layperson the analysis on the dataset \"+\" and the learning question\"+st.session_state['learning_question']+\" and conclusion base on the best model found \"+st.session_state['best_model_name']\n",
        "  ques += \". answer the learning question: \"+st.session_state['learning_question']\n",
        "  ans = call_gpt(ques)\n",
        "  return ans\n",
        "\n",
        "\n",
        "def prediction():\n",
        "        processed_input = {}\n",
        "        for col, value in st.session_state['user_input'].items():\n",
        "            try:\n",
        "                if pd.api.types.is_integer_dtype(st.session_state['column_types'][col]):\n",
        "                    processed_input[col] = int(value)\n",
        "                elif pd.api.types.is_float_dtype(st.session_state['column_types'][col]):\n",
        "                    processed_input[col] = float(value)\n",
        "                elif pd.api.types.is_datetime64_any_dtype(st.session_state['column_types'][col]):\n",
        "                    processed_input[col] = pd.to_datetime(value)\n",
        "                elif pd.api.types.is_object_dtype(st.session_state['column_types'][col]):\n",
        "                    processed_input[col] = value  # Assuming string\n",
        "            except ValueError:\n",
        "                st.error(f\"Invalid input for {col}. Please enter a valid {st.session_state['column_types'][col]} value.\")\n",
        "                return\n",
        "\n",
        "        user_input_df = pd.DataFrame([processed_input])\n",
        "        prediction = st.session_state['best_model'].predict(user_input_df)\n",
        "        pred_keys_lst = list(st.session_state['pred_val_dic'].keys())\n",
        "        pred_keys_lst_int = [int(k) for k in pred_keys_lst]\n",
        "        if st.session_state['pred_val_dic'] is not None and prediction[0] in pred_keys_lst_int:\n",
        "          if all(isinstance(key, str) for key in list(st.session_state['pred_val_dic'].keys())):\n",
        "            st.write(\"Prediction: \", st.session_state['pred_val_dic'][str(prediction[0])])  # Assuming binary classification\n",
        "          else:\n",
        "            st.write(\"Prediction: \", st.session_state['pred_val_dic'][prediction[0]])  # Assuming binary classification\n",
        "        else:\n",
        "          st.success(f'Prediction: {prediction[0]}')  # Assuming binary classification\n",
        "        if st.session_state['model_type'] == 'classification':\n",
        "          probability = st.session_state['best_model'].predict_proba(user_input_df)\n",
        "\n",
        "if st.session_state['df'] is not None and not st.session_state['df'].empty:\n",
        "  st.session_state['option'] = st.selectbox(\n",
        "   r\"$\\textsf{\\Large What kind of analize would you like to perform?}$\",\n",
        "   (\"Fast Analize\", \"Mid Analize\", \"Deep Analize\"),\n",
        "   index=None,\n",
        "   placeholder=\"Select contact method...\",\n",
        "  )\n",
        "\n",
        "  st.write(' Your Choice:', st.session_state['option'])\n",
        "\n",
        "  if st.session_state['option'] is not None:\n",
        "\n",
        "      #if st.button('Analyze', on_click=run_analysis):\n",
        "      if st.button('Analyze'):\n",
        "        st.session_state['analyze_button'] = True\n",
        "        full_analysis()\n",
        "\n",
        "        if st.session_state['option'] == 'Fast Analize':\n",
        "          for model_name in st.session_state['models_extracted']:\n",
        "            print(\"check model_name= \", model_name)\n",
        "            new_model_name = map_model_name(model_name, st.session_state['model_type'])\n",
        "\n",
        "            if new_model_name is not None:\n",
        "                model_name = new_model_name\n",
        "                break\n",
        "          st.session_state['models'] = st.session_state['models_dic']\n",
        "          model = st.session_state['models'][model_name]\n",
        "\n",
        "          pipe_steps = [('preprocessor', st.session_state['preprocessor']), ('model', model)]\n",
        "          st.session_state['pipe'] = Pipeline(pipe_steps)\n",
        "          st.session_state['pipe'].fit(st.session_state['X_train_processed'], st.session_state['y_train'])\n",
        "\n",
        "          y_test_predict = st.session_state['pipe'].predict(st.session_state['X_test_processed'])\n",
        "          if st.session_state['model_type'] == 'regression':\n",
        "             test_pred_score = r2_score(st.session_state['y_test'], y_test_predict)\n",
        "          elif st.session_state['model_type'] == 'classification':\n",
        "             test_pred_score = metrics.f1_score(st.session_state['y_test'], y_test_predict)\n",
        "\n",
        "          st.session_state['best_score'] = round(test_pred_score,3)\n",
        "          st.session_state['best_model'] = model\n",
        "          st.session_state['best_model_name'] = model_name\n",
        "          st.session_state['best_model_params'] = None\n",
        "          st.session_state['best_pipe'] = st.session_state['pipe']\n",
        "\n",
        "          st.session_state['metrics_dic'] = best_model_metrics(st.session_state['best_model'])\n",
        "          st.session_state['metric_explanation'] = check_model_metrics(st.session_state['best_model_name'], st.session_state['metrics_dic'])\n",
        "\n",
        "          st.session_state['output'] = f\"The best model found so far is {st.session_state['best_model_name']} with the score: {st.session_state['best_score']}. \\nexplanation:\\n {st.session_state['metric_explanation']}.\"\n",
        "\n",
        "        elif st.session_state['option'] == 'Mid Analize':\n",
        "            #st.session_state['iteration_num'] = 7\n",
        "            #st.session_state['iteration_num'] = 5\n",
        "            #st.session_state['iteration_num'] = 3\n",
        "            #st.session_state['iteration_num'] = 2\n",
        "            st.session_state['iteration_num'] = 1\n",
        "            st.session_state['models'] = st.session_state['models_dic']\n",
        "            st.session_state['output'] = full_grid_search()\n",
        "\n",
        "        elif st.session_state['option'] == 'Deep Analize':\n",
        "            #st.session_state['iteration_num'] = 15\n",
        "            st.session_state['iteration_num'] = 20\n",
        "            st.session_state['models'] = st.session_state['models_dic']\n",
        "            st.session_state['output'] = full_grid_search()\n",
        "        st.write(st.session_state['output'])\n",
        "\n",
        "      if st.button('Display'):\n",
        "        if st.session_state['analyze_button'] == False:\n",
        "          st.write(\"Click on the Analize button first\")\n",
        "        else:\n",
        "          display_results()\n",
        "\n",
        "      if st.button('Summary'):\n",
        "        if st.session_state['analyze_button'] == False:\n",
        "          st.write(\"Click on the Analize button first\")\n",
        "        else:\n",
        "          st.write('summary: ', get_summary())\n",
        "\n",
        "      if st.button('predict'):\n",
        "        if st.session_state['analyze_button'] == False:\n",
        "          st.write(\"Click on the Analize button first\")\n",
        "        else:\n",
        "          prediction()\n",
        "\n",
        "\n",
        "if st.session_state['df'] is not None and not st.session_state['df'].empty and st.session_state['prediction_value'] is not None:\n",
        "    if 'user_input' not in st.session_state:\n",
        "        st.session_state['user_input'] = {}\n",
        "    if 'column_types' not in st.session_state:\n",
        "        st.session_state['column_types'] = {}\n",
        "    st.session_state['column_types'] = st.session_state['df'][st.session_state['df'].columns.drop(st.session_state['prediction_value'])].dtypes\n",
        "\n",
        "    for col in st.session_state['df'].columns.drop(st.session_state['prediction_value']):\n",
        "        st.session_state['user_input'][col] = None\n",
        "        user_input_value = st.text_input(f\"Enter value for {col} ({st.session_state['column_types'][col]}): \", key=col)\n",
        "        st.session_state['user_input'][col] = user_input_value\n",
        "        while st.session_state['user_input'][col] == None:\n",
        "          print(\"check\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2WgUWztA3ftRgnU7qXnpSw8Ojp0_82hfZKY7Pehf1epbQEkz6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqaQS5ytQah6",
        "outputId": "7c97b0e6-400e-4967-ca28-1cd8b8be723d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjZ58dwyQc76",
        "outputId": "3d83c167-87eb-4e35-ddd3-4d6197762774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-27 20:32:21--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 18.205.222.128, 54.237.133.81, 54.161.241.46, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|18.205.222.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  39.1MB/s    in 0.3s    \n",
            "\n",
            "2024-02-27 20:32:22 (39.1 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13921656/13921656]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ba1uyscaQdZj",
        "outputId": "fc278722-21a7-40a1-a0a9-d8a16c54bcc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 8501 &')"
      ],
      "metadata": {
        "id": "fq_t1tMRQgyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels |python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGEEqHPIQi4l",
        "outputId": "130cc12c-cb91-4160-8eaa-e3b4e1b55038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/usr/lib/python3.10/json/__init__.py\", line 293, in load\n",
            "    return loads(fp.read(),\n",
            "  File \"/usr/lib/python3.10/json/__init__.py\", line 346, in loads\n",
            "    return _default_decoder.decode(s)\n",
            "  File \"/usr/lib/python3.10/json/decoder.py\", line 337, in decode\n",
            "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
            "  File \"/usr/lib/python3.10/json/decoder.py\", line 355, in raw_decode\n",
            "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
            "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install streamlit -q"
      ],
      "metadata": {
        "id": "QQZ0rvlpQlal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnjxVD8IQqU4",
        "outputId": "006bf39c-bbfe-4ca1-92dd-48e69404b4cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEazoupGQty0",
        "outputId": "2f43c65a-0eda-488f-82c2-cfb09911e8d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.69.222.120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJ9rifw7QuTY",
        "outputId": "69f0206b-8703-43e0-8927-118c6ee4b5b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[..................] / fetchMetadata: sill resolveWithNewModule localtunnel@2.0\u001b[0m\u001b[K\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.69.222.120:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.992s\n",
            "your url is: https://light-chicken-go.loca.lt\n",
            "2024-02-27 20:36:06.445231: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-27 20:36:06.445301: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-27 20:36:06.446868: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-27 20:36:06.456547: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-02-27 20:36:07.993380: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}